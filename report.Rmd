---
title: "Project Report"
output:   
  html_document:
    toc: true
    toc_float: true
    code_folding: hide
---

```{r setup, include=FALSE}
library(tidyverse)
library(maps)
#library(tigris)
library(dplyr)
library(leaflet)
library(sp)
library(ggmap)
library(maptools)
library(broom)
library(httr)
library(rgdal)
```

### Tweet Data

We opted to use the two datasets found [here](https://www.kaggle.com/manchunhui/us-election-2020-tweets) constructed from scraping Twitter for tweets either containing _#Biden_ (or _#JoeBiden_) or containing _#Trump_ (or _#DonaldTrump_) from Oct 15th through Nov 8th. 

Because the datasets were large we droped missing data and divided each dataset into two subsets to be able to store them within our repository.

```{r 50MB_goal, eval = FALSE}
read_csv("./datasets/hashtag_donaldtrump.csv") %>% 
  drop_na() %>%
  slice(seq(0.5 * n())) %>% 
  write.csv("./datasets/trump1.csv")

read_csv("./datasets/hashtag_donaldtrump.csv") %>% 
  drop_na() %>%
  slice(-seq(0.5 * n())) %>% 
  write.csv("./datasets/trump2.csv")
  
read_csv("./datasets/hashtag_joebiden.csv") %>% 
  drop_na() %>% 
  slice(seq(0.5 * n())) %>% 
  write.csv("./datasets/biden1.csv")

read_csv("./datasets/hashtag_joebiden.csv") %>% 
  drop_na() %>% 
  slice(-seq(0.5 * n())) %>% 
  write.csv("./datasets/biden2.csv")
```

Then, data was imported, merged, and cleaned, especially date of creation of the tweet, for further analysis. We further limited our analysis to tweets from users registered within the U.S. and with only one of the cadidates hashtags (i.e.: we excluded tweets containing both #Trump/#DonaldTrump and #Biden/#joeBiden).

```{r tweet_import, warning = FALSE, message = FALSE}
#Trump / #DonaldTrump tweets
trump_df = 
  merge(
    read_csv("./datasets/trump1.csv"),
    read_csv("./datasets/trump2.csv"),
    all = TRUE
  ) %>%
  select(!X1) %>% 
  separate(created_at, into = c("creation_date", "creation_time"), sep = " ") %>% 
  separate(creation_date, into = c("creation_year", "creation_month", "creation_day"), sep = "-") %>% 
  separate(user_join_date, into = c("join_date", "join_time"), sep = " ") %>% 
  separate(join_date, into = c("join_year", "join_month", "join_day"), sep = "-") %>% 
  mutate(hashtag = "Trump")

#Biden / #JoeBiden tweets
biden_df = 
  merge(
    read_csv("./datasets/biden1.csv"),
    read_csv("./datasets/biden2.csv"),
    all = TRUE
  ) %>%
  select(!X1) %>%  
  separate(created_at, into = c("creation_date", "creation_time"), sep = " ") %>% 
  separate(creation_date, into = c("creation_year", "creation_month", "creation_day"), sep = "-") %>% 
  separate(user_join_date, into = c("join_date", "join_time"), sep = " ") %>% 
  separate(join_date, into = c("join_year", "join_month", "join_day"), sep = "-") %>% 
  mutate(hashtag = "Biden")

# Subset of USA tweets from US-registered users only
tweets_usa =     # N = 140263 tweets
  merge(biden_df, trump_df, all = TRUE) %>% 
  filter(country == "United States of America") %>% 
  distinct(tweet, .keep_all = TRUE)   # N = 123635 (16628 tweets removed containing both hashtags)
```

The final dataframe `tweets_usa` have `r ncol(tweets_usa)` columns with the relevant variables used in this report as follows:

* `creation_year`, `creation_month`,`creation_day`, and `creation_time`. Date and time of tweet creation.
* `tweet`. Full tweet text.
* `likes`. Number of likes.
* `retweet_count`. Number of retweets.
* `user_followers_count`. Followers count on user account at the time of tweet creation.
* `lat` and `long`. Latitude and longitude parsed from user location.
* `city`, `state`, and `state_code. City and state from user location.
* `hashtag`. Whether tweet contains #Biden or #Trump.

Election stuff

```{r, message = FALSE}
election_df =
  read_csv("./datasets/president_county_candidate.csv") %>% 
  group_by(state, party) %>% 
  mutate(party_total = sum(total_votes)) %>% 
  ungroup() %>% 
  group_by(state) %>%
  mutate(state_winner = case_when(
                      party_total == max(party_total) ~ TRUE,
                      party_total != max(party_total) ~ FALSE),
         state_total = sum(total_votes)
  )

state_election_df = 
  election_df %>% 
  filter(state_winner == TRUE) %>% 
  select(state, candidate, party, party_total, state_total) %>% 
  distinct()
```

joining df

```{r}
main_tweets_usa = 
  left_join(tweets_usa, state_election_df, by = "state") %>% 
  rename(
    winner_candidate = candidate,
    winner_party = party
  )
```

## Plots
Characterizing tweets
TODO : use the same colors

```{r}
tweets_usa %>%
group_by(state, hashtag)%>%
summarise(count = n()) %>%  
ggplot() +
geom_col(aes(x=state, y=count, fill = hashtag ), position = "dodge") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
  labs (title = "Distribution of tweets across US states")
```


```{r}
tweets_usa %>% 
mutate(date = as.Date(paste(creation_month, creation_day, '2020', sep = "/"), format  = "%m/%d/%y")) %>%  
group_by (date, hashtag) %>%
summarise(count = n())  %>%
ggplot(aes(x=date,y=count,color = hashtag)) +
geom_smooth(method = "lm") +
geom_point(aes(x=date, y=count, color = hashtag)) +
geom_vline(xintercept = as.Date("2020-11-02")) +
theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
scale_x_date(date_breaks = '1 day') +
labs (title = "Distribution of tweets over time", 
        x = "Day",
        y = "Number of tweets")
```

We see that there is an increase in tweets over time with Biden having less tweets overall (albeit not statistically significant)


```{r}

#tweets_usa %>%
#select (user_followers_count) %>%
#  arrange(desc(user_followers_count))

tweets_usa %>%
group_by(hashtag)%>%
filter (retweet_count>0 & likes>0) %>%
filter (user_followers_count<=3750110) %>%# filter outliers  
mutate(user_followers_count = user_followers_count/1000) %>%
ggplot() +
geom_point(aes(x= user_followers_count , y= retweet_count, color = hashtag, size = likes  )) +
  labs (title = "How tweets are shared and liked depending on the number of followers", 
        x = "Numer of users, thousands",
        y = "Number of retweets")
```
 



```{r}

usa_map <- map_data("state")

tweet_map <- tweets_usa %>%
group_by(state, hashtag) %>%
summarise(count = n(),
          likes = sum(likes)) %>%  
mutate (likes_tweets = likes*count,
        region = tolower(state)) %>%
select (region, hashtag, likes_tweets)  %>%
pivot_wider(names_from = "hashtag",
            values_from = "likes_tweets")  %>%
mutate(top = case_when(Biden>coalesce(Trump,0) ~ "Biden",
                       Trump>Biden ~ "Trump"))


states_tweet_map <- left_join(usa_map, tweet_map)


    ggplot(data = states_tweet_map,
            aes(x = long, y = lat,
                group = group, fill = top)) +
    geom_polygon(color = "gray90", size = 0.1) +
    labs(title ="Most popular tweets across states")
  
```




for NYC map over boroughs + plot them over map 
```{r, eval = FALSE}
    NY <-tweets_usa %>% 
    filter (state_code == 'NY' & city == 'New York') %>%
    filter (likes>10 & retweet_count>10) %>%
    select (lat, long, hashtag, likes, retweet_count)

r <- GET('http://data.beta.nyc//dataset/0ff93d2d-90ba-457c-9f7e-39e47bf2ac5f/resource/35dd04fb-81b3-479b-a074-a27a37888ce7/download/d085e2f8d0b54d4590b1e7d1f35594c1pediacitiesnycneighborhoods.geojson')
nyc_neighborhoods <- readOGR(content(r,'text'), 'OGRGeoJSON', verbose = F)

NY_points <- NY
sp::coordinates(NY_points) <- ~long + lat
proj4string(NY_points) <- proj4string(nyc_neighborhoods)
matches <- over(NY_points, nyc_neighborhoods)
points <- cbind(NY, matches)
map_data <- geo_join(nyc_neighborhoods, points, "neighborhood", "neighborhood")


leaflet(map_data) %>%
  addTiles() %>% 
  addPolygons(popup = ~hashtag) %>% 
  addMarkers(~long, ~lat, popup = ~hashtag, data = points) %>%
  addProviderTiles("CartoDB.Positron") %>%
  setView(-75, 40.75, zoom = 10)

```