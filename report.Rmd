---
title: "Project Report"
output:   
  html_document:
    toc: true
    toc_float: true
    code_folding: hide
---

```{r setup, include=FALSE}
library(tidyverse)
library(maps)
#library(tigris)
library(dplyr)
library(leaflet)
library(sp)
library(ggmap)
library(maptools)
library(broom)
library(httr)
library(rgdal)

colors = c("dark red", "dark blue")
names(colors) = c("Trump", "Biden")
```


# Motivation: 
Questions to consider: Provide an overview of the project goals and motivation; 

US polls have been the center of attention in recent presidential elections due to the fact that the results have not accurately reflected predictions ([US PresidentialElection Polls Failure](https://news.northeastern.edu/2020/11/04/the-polls-were-still-way-off-in-the-2020-election-even-after-accounting-for-2016s-errors/)). 
Pollsters suggest that the evidence of the 2016 and 2020 elections will improve polling models. 

On the other hand, the evolution of social media has been tied to political behavior such as voting ([Twitter as Indicator for Political Behavior](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0079449)). Since the public is quickly losing the trust of polls for political elections, researchers are now exploring whether social media activity can be utilized to assess offline political behavior ([Can Twitter Data Forecast Presidential Elections?](https://www.tandfonline.com/doi/full/10.1080/19475683.2020.1829704)). We believe that extracting online social networking environments can shed light on communication patterns and political preferences of individuals.

We hypothesize that social media can be beneficial in identifying electoral behaviors. We want to use Twitter (specifically geocoding tweets referencing presidential candidates) to identify the political affiliation to that region.    

## *Related work*
Things to consider: Anything that inspired you, such as a paper, a web site, or something we discussed in class.

Sources!

***

# Initial questions: 
Questions to consider: What questions are you trying to answer? How did these questions evolve over the course of the project? What new questions did you consider in the course of your analysis?

In this project, we will:

* Compare tweets of #Biden/#Trump by county/state and US polls to see if tweets are more accurate of predicting presidential results. We will make a choropleth map to visualize states that tweeted #Biden and #Trump and compare it to the 2020 election outcome. We will do the same with the polls. 

* Explore possible correlations between Twitter election-related post and their features (e.g. tweet share, emoji) and election results.

Suggested Plot/Charts at Proposal:  

1) Distribution of #Biden/#Trump tweets by state;  
2) US polls prediction of winning candidate by state   
3) Graph of Current Votes by state for US Election Results  
4) Interactive visualisization of tweets features and content and their distribution by state

Projected Challenges during Proposal: 

1) Cleaning up tweets free-text or identifying meaningful variables in free text
2) Processing the poll data to allow polls/tweets comparison
2) Identifying proper model for correlation 
3) Building an interactive visualization embedded into a web site
4) Handling large volume of data

***

# Data Sources
Questions to consider: scraping method, cleaning, etc.

## *2020 Polls Dataset* 
[2020 Election Forecast](https://github.com/fivethirtyeight/data/tree/master/election-forecasts-2020)

Polls were summarized by taking the mean of the expected votes per candidate in the 3 days leading up to the election (November 1-3) prior to displaying data visualization. 

## *2020 Election Results Dataset* 
[2020 Election](https://www.kaggle.com/unanimad/us-election-2020)

Insert cleaning, scraping, methods here 

## *US Regional Dataset* 
[US States by Region](https://www.kaggle.com/omer2040/usa-states-to-region)

Insert cleaning, scraping, methods here 

## *2020 US Election Twitter Dataset* 
[US Election Twitter](https://www.kaggle.com/manchunhui/us-election-2020-tweets)

Insert cleaning, scraping, methods here 

***

# Exploratory analysis:
Things to consider: Visualizations, summaries, and exploratory statistical analyses. Justify the steps you took, and show any major changes to your ideas.

## *2020 Polls Visualisation* 

### Data Wrangling
Insert text here 

### Exploratory Analysis
Insert text here 

## *2020 Election Visualisation* 

### Data Wrangling
Insert text here 

### Exploratory Analysis
Insert text here 

## *2020 Election Tweet Data & Visualisation* 

### Data Wrangling

We opted to use the two datasets found [here](https://www.kaggle.com/manchunhui/us-election-2020-tweets) constructed from scraping Twitter for tweets either containing _#Biden_ (or _#JoeBiden_) or containing _#Trump_ (or _#DonaldTrump_) from Oct 15th through Nov 8th. 

Because the datasets were large we droped missing data and divided each dataset into two subsets to be able to store them within our repository.

```{r 50MB_goal, eval = FALSE}
read_csv("./datasets/hashtag_donaldtrump.csv") %>% 
  drop_na() %>%
  slice(seq(0.5 * n())) %>% 
  write.csv("./datasets/trump1.csv")

read_csv("./datasets/hashtag_donaldtrump.csv") %>% 
  drop_na() %>%
  slice(-seq(0.5 * n())) %>% 
  write.csv("./datasets/trump2.csv")
  
read_csv("./datasets/hashtag_joebiden.csv") %>% 
  drop_na() %>% 
  slice(seq(0.5 * n())) %>% 
  write.csv("./datasets/biden1.csv")

read_csv("./datasets/hashtag_joebiden.csv") %>% 
  drop_na() %>% 
  slice(-seq(0.5 * n())) %>% 
  write.csv("./datasets/biden2.csv")
```

Then, data was imported, merged, and cleaned, especially date of creation of the tweet, for further analysis. We further limited our analysis to tweets from users registered within the U.S. and with only one of the cadidates hashtags (i.e.: we excluded tweets containing both #Trump/#DonaldTrump and #Biden/#JoeBiden).

```{r tweet_import, warning = FALSE, message = FALSE}
#Trump / #DonaldTrump tweets
trump_df = 
  merge(
    read_csv("./datasets/trump1.csv"),
    read_csv("./datasets/trump2.csv"),
    all = TRUE
  ) %>%
  select(!X1) %>% 
  separate(created_at, into = c("creation_date", "creation_time"), sep = " ") %>% 
  separate(creation_date, into = c("creation_year", "creation_month", "creation_day"), sep = "-") %>% 
  separate(user_join_date, into = c("join_date", "join_time"), sep = " ") %>% 
  separate(join_date, into = c("join_year", "join_month", "join_day"), sep = "-") %>% 
  mutate(hashtag = "Trump")

#Biden / #JoeBiden tweets
biden_df = 
  merge(
    read_csv("./datasets/biden1.csv"),
    read_csv("./datasets/biden2.csv"),
    all = TRUE
  ) %>%
  select(!X1) %>%  
  separate(created_at, into = c("creation_date", "creation_time"), sep = " ") %>% 
  separate(creation_date, into = c("creation_year", "creation_month", "creation_day"), sep = "-") %>% 
  separate(user_join_date, into = c("join_date", "join_time"), sep = " ") %>% 
  separate(join_date, into = c("join_year", "join_month", "join_day"), sep = "-") %>% 
  mutate(hashtag = "Biden")

# Subset of USA tweets from US-registered users only
tweets_usa =     # N = 140263 tweets
  merge(biden_df, trump_df, all = TRUE) %>% 
  filter(country == "United States of America") %>% 
  distinct(tweet, .keep_all = TRUE)   # N = 123635 (16628 tweets removed containing both hashtags)
```

The dataframe used in this section `tweets_usa` have `r ncol(tweets_usa)` columns with the relevant variables used in this report as follows:

* `creation_year`, `creation_month`,`creation_day`, and `creation_time`. Date and time of tweet creation.
* `tweet`. Full tweet text.
* `likes`. Number of likes.
* `retweet_count`. Number of retweets.
* `user_followers_count`. Followers count on user account at the time of tweet creation.
* `lat` and `long`. Latitude and longitude parsed from user location.
* `city`, `state`, and `state_code`. City and state from user location.
* `hashtag`. Whether tweet contains #Biden or #Trump.

We added state election-related variables generated by the 2020 election results dataset found [here](https://www.kaggle.com/unanimad/us-election-2020) to the `tweets_usa` dataframe. 
The final dataframe `main_tweets_usa` used to construct _word clouds_ - see below - has the following extra variables:

* `winner_party` and `winner_candidate`. Party and candidate who won the election in each state.
* `party_total`. Total number of votes for the party.
* `state_total`. Total number of votes for the party in each state.

```{r, message = FALSE}
election_df =
  read_csv("./datasets/president_county_candidate.csv") %>% 
  group_by(state, party) %>% 
  mutate(party_total = sum(total_votes)) %>% 
  ungroup() %>% 
  group_by(state) %>%
  mutate(state_winner = case_when(
                      party_total == max(party_total) ~ TRUE,
                      party_total != max(party_total) ~ FALSE),
         state_total = sum(total_votes)
  )

state_election_df = 
  election_df %>% 
  filter(state_winner == TRUE) %>% 
  select(state, candidate, party, party_total, state_total) %>% 
  distinct()

main_tweets_usa = 
  left_join(tweets_usa, state_election_df, by = "state") %>% 
  rename(
    winner_candidate = candidate,
    winner_party = party
  )
```

### Exploratory Analysis

We then explored the data to characterize the distribution of tweets across different locations and time to see if there were any patterns or trends.
We first plot the distribution of Biden and Trump tweets across states.

```{r, message = FALSE}
tweets_usa %>%
group_by(state, hashtag)%>%
summarise(count = n()) %>%  
ggplot() +
geom_col(aes(x=state, y=count, fill = hashtag ), position = "dodge") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
  labs (title = "Distribution of tweets across US states") +
scale_fill_manual(values = colors)
```

We observe abnormal distribution of tweets density across states with California and New York being the most active states with more than 15,000 tweets during October-November. That aligns with our prior expectations. Other active states included Florida and Texas, which was unexpected. Overall, more urban states tended to be more active on discussing candidates.
Almost in each state Trump gained more attention with Ohio being the only exception.

We then looked at the distribution of tweets across time and attempted to estimate if there is a significant difference in hashtag use over time between the two candidates.

```{r, message = FALSE}
tweets_usa %>% 
mutate(date = as.Date(paste(creation_month, creation_day, '2020', sep = "/"), format  = "%m/%d/%y")) %>%  
group_by (date, hashtag) %>%
summarise(count = n())  %>%
ggplot(aes(x=date,y=count,color = hashtag)) +
geom_smooth(method = "lm") +
geom_point(aes(x=date, y=count, color = hashtag)) +
geom_vline(xintercept = as.Date("2020-11-02")) +
theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
scale_x_date(date_breaks = '1 day') +
labs (title = "Distribution of tweets over time", 
        x = "Day",
        y = "Number of tweets") +
scale_color_manual(values = colors)
```

We saw that there is an increase in tweets over time with a peak two days aafter the election day when the preliminary results were revealed. Biden was mentioned less frequently at each time point except for Novemeber 4. Although the difference wasn't statistically significant, we observed several outliers with unusually high rate of tweets with #Trump hashtag (October 16 and October 23).


We saw that Trump was mentioned more often. But who talked about him? As the next step, we wanted to look at the correlation between user popularity (authority) and candidates this user mentions. As a proxy for user's popularity we selected the number of followers. to characterize tweets we multiplied the number of likes per tweet with a number of re-shares to reflect on how much attention a tweet gained.
We plotted the data with number of user's followers on X axis and number of re-shares on Y axis. Each dot represented a tweet and its size correlated with the number of likes. 

```{r, message = FALSE}

#tweets_usa %>%
#select (user_followers_count) %>%
#  arrange(desc(user_followers_count))

tweets_usa %>%
group_by(hashtag)%>%
filter (retweet_count>0 & likes>0) %>%
filter (user_followers_count<=3750110) %>%# filter outliers  
mutate(user_followers_count = user_followers_count/1000) %>%
ggplot() +
geom_point(aes(x= user_followers_count , y= retweet_count, color = hashtag, size = likes  )) +
  labs (title = "How tweets are shared and liked depending on the number of followers", 
        x = "Numer of users, thousands",
        y = "Number of retweets") +
scale_color_manual(values = colors)
```
 

We observed that the users with a lot of followers tweeted about Trump more than about Biden. On the other hand, those tweets were not popular - they were not liked or re-shared. On the other hand, there are popular tweets related to Biden with a lot of likes and re-tweets (in the middle of the plot). Regardless of the number of followers or re-tweets, messages about Trump are significantly less liked.



Interestingly enough, we saw that California not only talked about Trump more, but also re-shared and liked tweets with Trump more (regardless of the tone of those tweets). New York, on the other hand, liked and shared tweets primarily about Biden. In fact, no strong correlation between tweet behavior and electorate attitude can be observed.

```{r, message = FALSE}

usa_map <- map_data("state")

tweet_map <- tweets_usa %>%
group_by(state, hashtag) %>%
summarise(count = n(),
          likes = sum(likes)) %>%  
mutate (likes_tweets = likes*count,
        region = tolower(state)) %>%
select (region, hashtag, likes_tweets)  %>%
pivot_wider(names_from = "hashtag",
            values_from = "likes_tweets")  %>%
mutate(top = case_when(Biden>coalesce(Trump,0) ~ "Biden",
                       Trump>Biden ~ "Trump"))


states_tweet_map <- left_join(usa_map, tweet_map)


    ggplot(data = states_tweet_map,
            aes(x = long, y = lat,
                group = group, fill = top)) +
    geom_polygon(color = "gray90", size = 0.1) +
    labs(title ="Most popular tweets across states") +
    scale_fill_manual(values = colors)
  
```


Given that New York was among the most active states, we wanted to look at the popular tweets in different boroughs. Unfortunately, the data only provided rough estimate of user location.
Nevertheless, Bronx, Brooklyn and a part of Queens talked about Trump more.

```{r, message = FALSE, warning = FALSE, eval = FALSE}
    NY <-tweets_usa %>% 
    filter (state_code == 'NY' & city == 'New York') %>%
    filter (likes>10 & retweet_count>10) %>%
    select (lat, long, hashtag, likes, retweet_count)

r <- GET('http://data.beta.nyc//dataset/0ff93d2d-90ba-457c-9f7e-39e47bf2ac5f/resource/35dd04fb-81b3-479b-a074-a27a37888ce7/download/d085e2f8d0b54d4590b1e7d1f35594c1pediacitiesnycneighborhoods.geojson')
nyc_neighborhoods <- readOGR(content(r,'text'), 'OGRGeoJSON', verbose = F)

NY_points <- NY
sp::coordinates(NY_points) <- ~long + lat
proj4string(NY_points) <- proj4string(nyc_neighborhoods)
matches <- over(NY_points, nyc_neighborhoods)
points <- cbind(NY, matches)
map_data <- geo_join(nyc_neighborhoods, points, "neighborhood", "neighborhood")


leaflet(map_data) %>%
  addTiles() %>% 
  addPolygons(popup = ~hashtag) %>% 
  addMarkers(~long, ~lat, popup = ~hashtag, data = points) %>%
  addProviderTiles("CartoDB.Positron") %>%
  setView(-75, 40.75, zoom = 10)

```

*** 

# Additional analysis:
Things to consider: If you undertake formal statistical analyses, describe these in detail

## *Word Clouds*

We constructed an interactive shinyapp to display the most frequent words within tweets selected according to user-defined parameters in the form of word clouds. We used the `tm` package to clean tweet text of non-textual elements as well as stop words (both english and spanish).
We developed a custom function `wordcount_df` that incorporates functions of this package and _tidyverse_ elements to construct dataframes with a `word` and a `freq` column. This dataframe is the raw material for the `wordcloud2` package. It has several functionalities to create word clouds and with the _renderwordcloud2_ function it can be incorporated into a [shinyapp©](wc_redirect.html).

Here is the code used to generate the app and a screenshot of the final product:
```{r app, eval = FALSE}
if (require(shiny)) {
  
  library(tidyverse)
  library(flexdashboard)
  library(shiny)
  library(wordcloud2)
  library(tm)
  library(rsconnect)
  
  set.seed(1)

  trump_df = 
    merge(
      read_csv("./datasets/trump1.csv"),
      read_csv("./datasets/trump2.csv"),
      all = TRUE
    ) %>%
    select(!X1) %>%
    mutate(hashtag = "Trump")
  
  biden_df = 
    merge(
      read_csv("./datasets/biden1.csv"),
      read_csv("./datasets/biden2.csv"),
      all = TRUE
    ) %>%
    select(!X1) %>%
    mutate(hashtag = "Biden")
  
  tweets_usa =
    merge(biden_df, trump_df, all = TRUE) %>% 
    filter(country == "United States of America")
  
  election_df =
    read_csv("./datasets/president_county_candidate.csv") %>% 
    group_by(state, party) %>% 
    mutate(party_total = sum(total_votes)) %>% 
    ungroup() %>% 
    group_by(state) %>%
    mutate(state_winner = case_when(
      party_total == max(party_total) ~ TRUE,
      party_total != max(party_total) ~ FALSE),
      state_total = sum(total_votes)
    )
  
  state_election_df = 
    election_df %>% 
    filter(state_winner == TRUE) %>% 
    select(state, candidate, party, party_total, state_total) %>% 
    distinct()
  
  main_tweets_usa = 
    left_join(tweets_usa, state_election_df, by = "state") %>% 
    rename(
      winner_candidate = candidate,
      winner_party = party
    ) %>% 
    select(tweet, hashtag, winner_party)

  wordcount_df = function(df) {
    
    if (!is.data.frame(df)) {
      stop("Input must be a dataframe")
    }
    
    df_text = 
      df %>% 
      slice_head(n = 1000) %>% 
      select(tweet) %>%
      mutate(
        tweet = gsub("#[[:alpha:]]*", "", tweet),
        tweet = gsub("@[[:alpha:]]*", "", tweet),
        tweet = gsub("https\\S*", "", tweet),
        tweet = gsub("http\\S*", "", tweet),
        tweet = gsub("@\\S*", "", tweet),
        tweet = gsub("amp", "", tweet),
        tweet = gsub("[\r\n]", "", tweet),
        tweet = gsub("[0-9]", "", tweet),
        tweet = gsub("[[:punct:]]", "", tweet)
      )
    
    docs_df =
      Corpus(VectorSource(df_text)) %>% 
      tm_map(removeNumbers) %>%
      tm_map(removePunctuation) %>%
      tm_map(stripWhitespace) %>% 
      tm_map(content_transformer(tolower)) %>% 
      tm_map(removeWords, stopwords("english")) %>% 
      tm_map(removeWords, stopwords("spanish"))
    
    words_df = 
      TermDocumentMatrix(docs_df) %>% 
      as.matrix() %>% 
      rowSums() %>% 
      sort(decreasing = TRUE)
    
    word_count_df = data.frame(word = names(words_df),freq = words_df)
    
    return(word_count_df)
    
  }
  
  # Defining the UI
  
  ui =
    fluidPage(
    
      titlePanel("Election Tweets Explorer"),
      
      sidebarPanel(
        
      radioButtons(
        "Hashtag_choice",
        h4("Tweets containing..."),
        inline = TRUE,
        choiceNames = list("#Biden/#JoeBiden", "#Trump/#DonaldTrump", "Either"),
        choiceValues = list("Biden", "Trump", "Both"),
        selected = "Both"
      ),
      
      radioButtons(
        "Party_choice",
        h4("Tweets from states won by..."),
        inline = TRUE,
        choiceNames = list("Democrats", "Republicans", "All States"),
        choiceValues = list("DEM", "REP", "Both"),
        selected = "Both"
      ),
      
      sliderInput(
        "cut_top",
        h4("Number of words removed from top..."),
        min = 0, max = 10,
        value = 3
      ),
      
      h6("*Most frequent words are common among various parameters"),
      
      textInput(
        "word", 
        h4("Tweets containing specific text:"), 
        value = "", 
        width = NULL, 
        placeholder = NULL)),
      
      mainPanel(wordcloud2Output('wordcloud2'), width = 8)
    )
  
  
  # Defining the server code
  
  server = 
    function(input, output) {
    
      output$wordcloud2 <- renderWordcloud2({
      
        if (input[["word"]] == "") {
      
            if (input[["Hashtag_choice"]] == "Both" & input[["Party_choice"]] == "Both") {
      
                    main_tweets_usa %>% 
                      wordcount_df() %>% 
                      slice_tail(n = nrow(.) - input[["cut_top"]]) %>%
                      wordcloud2(size = 1, color = 'random-dark', ellipticity = 1)
      
            } else if (input[["Hashtag_choice"]] != "Both" & input[["Party_choice"]] != "Both") {
      
                    main_tweets_usa %>% 
                      filter(
                        hashtag == input[["Hashtag_choice"]],
                        winner_party == input[["Party_choice"]]
                      ) %>% 
                      wordcount_df() %>% 
                      slice_tail(n = nrow(.) - input[["cut_top"]]) %>%
                      wordcloud2(size = 1, color = 'random-dark', ellipticity = 1)
      
            } else if (input[["Hashtag_choice"]] == "Both" & input[["Party_choice"]] != "Both") {
        
                       main_tweets_usa %>% 
                         filter(
                           winner_party == input[["Party_choice"]]
                         ) %>% 
                         wordcount_df() %>% 
                         slice_tail(n = nrow(.) - input[["cut_top"]]) %>%
                         wordcloud2(size = 1, color = 'random-dark', ellipticity = 1)
        
            } else if (input[["Hashtag_choice"]] != "Both" & input[["Party_choice"]] == "Both") {
             
                        main_tweets_usa %>% 
                          filter(
                            hashtag == input[["Hashtag_choice"]]
                          ) %>% 
                          wordcount_df() %>% 
                          slice_tail(n = nrow(.) - input[["cut_top"]]) %>%
                          wordcloud2(size = 1, color = 'random-dark', ellipticity = 1)
             
        }
      
        } else {
      
            if (input[["Hashtag_choice"]] == "Both" & input[["Party_choice"]] == "Both") {
              
                        main_tweets_usa %>%
                         filter(str_detect(tweet, input[["word"]])) %>% 
                         wordcount_df() %>% 
                         slice_tail(n = nrow(.) - input[["cut_top"]]) %>%
                         wordcloud2(size = 1, ellipticity = 1, color = 'random-dark')
              
            } else if (input[["Hashtag_choice"]] != "Both" & input[["Party_choice"]] != "Both") {
              
                        main_tweets_usa %>% 
                          filter(
                            str_detect(tweet, input[["word"]]),
                            hashtag == input[["Hashtag_choice"]],
                            winner_party == input[["Party_choice"]]
                          ) %>% 
                          wordcount_df() %>% 
                          slice_tail(n = nrow(.) - input[["cut_top"]]) %>%
                          wordcloud2(size = 1, color = 'random-dark', ellipticity = 1)
              
            } else if (input[["Hashtag_choice"]] == "Both" & input[["Party_choice"]] != "Both") {
              
                         main_tweets_usa %>% 
                           filter(
                             str_detect(tweet, input[["word"]]),
                             winner_party == input[["Party_choice"]]
                           ) %>% 
                           wordcount_df() %>% 
                           slice_tail(n = nrow(.) - input[["cut_top"]]) %>%
                           wordcloud2(size = 1, color = 'random-dark', ellipticity = 1)
              
            } else if (input[["Hashtag_choice"]] != "Both" & input[["Party_choice"]] == "Both") {
              
                        main_tweets_usa %>% 
                          filter(
                            str_detect(tweet, input[["word"]]),
                            hashtag == input[["Hashtag_choice"]]
                          ) %>% 
                          wordcount_df() %>% 
                          slice_tail(n = nrow(.) - input[["cut_top"]]) %>%
                          wordcloud2(size = 1, color = 'random-dark', ellipticity = 1)
        
            }
      
          }  
        })
      }

    shinyApp(ui = ui, server = server)
    
  }
```

![](figures/wordcloud.png)

Users can explore tweet's most frequent words by hashtag, by twitter user account original state's winner party, or by specific text present within the tweet. Because the most frequent words are common among various selected parameters, we opted to allow users to remove a certain number of words from the top of the list. By doing so, it is possible to observe more sensitive differences among tweets.

***

# Discussion: 
Questions to consider: What were your findings? Are they what you expect? What insights into the data can you make?

